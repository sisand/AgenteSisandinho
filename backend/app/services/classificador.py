import logging
import re
from transformers import pipeline
import torch

logger = logging.getLogger(__name__)

# Detecta se h√° GPU dispon√≠vel
device = 0 if torch.cuda.is_available() else -1

# ===================================================================
# üëá IN√çCIO DA MUDAN√áA üëá
# ===================================================================

# O "endere√ßo" do modelo agora aponta para o seu reposit√≥rio no Hugging Face Hub
MODEL_ID = "sisand/classificador-sisandinho"

# Carrega o pipeline de classifica√ß√£o com o modelo da nuvem
logger.info(f"üîç Carregando modelo '{MODEL_ID}' do Hugging Face Hub...")
classificador_pipeline = None # Inicializa como None
try:
    # A biblioteca transformers usar√° o MODEL_ID para baixar e carregar o modelo.
    # Se o token HUGGING_FACE_HUB_TOKEN estiver configurado no Render, ele o usar√°
    # para acessar seu reposit√≥rio privado.
    classificador_pipeline = pipeline(
        "text-classification",
        model=MODEL_ID,
        device=device,
        top_k=3
    )
    logger.info("‚úÖ Modelo do Hub carregado com sucesso.")
except Exception as e:
    logger.error(f"‚ùå FALHA CR√çTICA AO CARREGAR MODELO DO HUB: {e}")
    # Se o modelo n√£o puder ser carregado, a aplica√ß√£o ainda pode subir,
    # mas a fun√ß√£o de classifica√ß√£o retornar√° um valor padr√£o.

# ===================================================================
# üëÜ FIM DA MUDAN√áA üëÜ
# ===================================================================


# Fun√ß√£o para normalizar texto (nenhuma mudan√ßa aqui)
def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = text.strip()
    return text

# Fun√ß√£o principal de classifica√ß√£o (pequeno ajuste de seguran√ßa)
async def classificar_pergunta(pergunta: str) -> str:
    # Adicionamos uma verifica√ß√£o para garantir que o pipeline foi carregado
    if not classificador_pipeline:
        logger.error("Pipeline de classifica√ß√£o n√£o est√° dispon√≠vel. Retornando 'geral'.")
        return "geral"

    pergunta_normalizada = normalize_text(pergunta)

    try:
        logger.info(f"üß† Classificando pergunta com modelo fine-tuneado: {pergunta}")
        resultado = classificador_pipeline(pergunta, truncation=True)
        logger.info(f"üéØ Resultado classifica√ß√£o: {resultado}")

        melhor_resultado = resultado[0][0]
        best_label = melhor_resultado['label']
        best_score = melhor_resultado['score']

        logger.info(f"‚úÖ Classifica√ß√£o final: {best_label} (confian√ßa: {best_score:.2f})")

        LIMIAR_CONFIANCA = 0.30
        if best_score < LIMIAR_CONFIANCA:
            logger.warning(f"‚ö†Ô∏è Confian√ßa baixa ({best_score:.2f}), retornando 'geral'")
            return "geral"

        return best_label.lower()
    except Exception as e:
        logger.error(f"‚ùå Erro na classifica√ß√£o: {e}")
        return "geral"