# app/services/classificador.py

import logging
import re
from transformers import pipeline, Pipeline
import torch
from typing import Optional

logger = logging.getLogger(__name__)

# Detecta se h√° GPU dispon√≠vel, caso contr√°rio usa a CPU
device = 0 if torch.cuda.is_available() else -1

# O "endere√ßo" do modelo agora aponta para o seu reposit√≥rio no Hugging Face Hub
MODEL_ID = "sisand/classificador-sisandinho"

# Carrega o pipeline de classifica√ß√£o com o modelo da nuvem
logger.info(f"üîç A carregar o modelo '{MODEL_ID}' do Hugging Face Hub...")
classificador_pipeline: Optional[Pipeline] = None # Inicializa como None para seguran√ßa
try:
    # A biblioteca transformers usar√° o MODEL_ID para descarregar e carregar o modelo.
    # Se o seu reposit√≥rio for privado, √© necess√°rio configurar um token de acesso.
    classificador_pipeline = pipeline(
        "text-classification",
        model=MODEL_ID,
        device=device,
        top_k=3 # Retorna as 3 categorias mais prov√°veis
    )
    logger.info("‚úÖ Modelo do Hub carregado com sucesso.")
except Exception as e:
    logger.error(f"‚ùå FALHA CR√çTICA AO CARREGAR O MODELO DO HUB: {e}")
    # Se o modelo n√£o puder ser carregado, a aplica√ß√£o ainda pode subir,
    # mas a fun√ß√£o de classifica√ß√£o retornar√° um valor padr√£o.


def normalizar_texto(texto: str) -> str:
    """Normaliza o texto para a classifica√ß√£o."""
    texto = texto.lower()
    texto = re.sub(r'[^\w\s]', '', texto)
    texto = texto.strip()
    return texto


async def classificar_pergunta(pergunta: str) -> str:
    """
    Classifica a pergunta do usu√°rio usando o modelo do Hugging Face.
    Retorna a categoria com maior pontua√ß√£o ou 'geral' se a confian√ßa for baixa.
    """
    # Verifica√ß√£o de seguran√ßa para garantir que o pipeline foi carregado
    if not classificador_pipeline:
        logger.error("O pipeline de classifica√ß√£o n√£o est√° dispon√≠vel. A retornar 'geral'.")
        return "geral"

    texto_normalizado = normalizar_texto(pergunta)

    try:
        logger.info(f"üß† A classificar pergunta com o modelo fine-tuneado: {pergunta}")
        resultado = classificador_pipeline(texto_normalizado, truncation=True)
        logger.info(f"üéØ Resultado da classifica√ß√£o: {resultado}")

        melhor_resultado = resultado[0][0]
        melhor_categoria = melhor_resultado['label']
        melhor_pontuacao = melhor_resultado['score']

        logger.info(f"‚úÖ Classifica√ß√£o final: {melhor_categoria} (confian√ßa: {melhor_pontuacao:.2f})")

        LIMIAR_CONFIANCA = 0.01
        if melhor_pontuacao < LIMIAR_CONFIANCA:
            logger.warning(f"‚ö†Ô∏è Confian√ßa baixa ({melhor_pontuacao:.2f}), a retornar 'geral'")
            return "geral"

        return melhor_categoria.lower()
    except Exception as e:
        logger.error(f"‚ùå Erro durante a classifica√ß√£o: {e}")
        return "geral"