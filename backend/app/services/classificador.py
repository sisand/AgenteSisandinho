import logging
import re
from transformers import pipeline
import torch

logger = logging.getLogger(__name__)

# Detecta se hÃ¡ GPU disponÃ­vel
device = 0 if torch.cuda.is_available() else -1

# Caminho para o modelo fine-tuneado
CAMINHO_MODELO_FINE_TUNE = "modelos/classificador_finetune"

# Carrega o pipeline de classificaÃ§Ã£o com o modelo treinado
logger.info("ðŸ” Carregando modelo fine-tuneado...")
classificador_pipeline = pipeline(
    "text-classification",
    model=CAMINHO_MODELO_FINE_TUNE,
    device=device,
    top_k=3
)
logger.info("âœ… Modelo fine-tuneado carregado com sucesso.")

# FunÃ§Ã£o para normalizar texto
def normalize_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = text.strip()
    return text

# FunÃ§Ã£o principal de classificaÃ§Ã£o
async def classificar_pergunta(pergunta: str) -> str:
    pergunta_normalizada = normalize_text(pergunta)

    try:
        # ClassificaÃ§Ã£o com o modelo fine-tuneado
        logger.info(f"ðŸ§  Classificando pergunta com modelo fine-tuneado: {pergunta}")
        resultado = classificador_pipeline(pergunta, truncation=True)

        logger.info(f"ðŸŽ¯ Resultado classificaÃ§Ã£o: {resultado}")

        # Se vier como lista de listas (por causa do top_k)
        melhor_resultado = resultado[0][0]

        best_label = melhor_resultado['label']
        best_score = melhor_resultado['score']

        logger.info(f"âœ… ClassificaÃ§Ã£o final: {best_label} (confianÃ§a: {best_score:.2f})")

        # Fallback opcional: se confianÃ§a for muito baixa, retorna "geral"
        LIMIAR_CONFIANCA = 0.30
        if best_score < LIMIAR_CONFIANCA:
            logger.warning(f"âš ï¸ ConfianÃ§a baixa ({best_score:.2f}), retornando 'geral'")
            return "geral"

        return best_label.lower()
    except Exception as e:
        logger.error(f"âŒ Erro na classificaÃ§Ã£o: {e}")
        return "geral"
