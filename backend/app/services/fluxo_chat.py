import logging
import time
from typing import Optional, List, Dict

from app.services.classificador import classificar_pergunta
from app.services.gerador_resposta import gerar_resposta
from app.core.clients import get_supabase_client
from app.core.clients import generate_chat_completion, gerar_embedding_openai, buscar_artigos_por_embedding



logger = logging.getLogger(__name__)

# üî• Hist√≥rico em mem√≥ria (substituir futuramente por Redis, banco, etc.)
_historico_conversas: Dict[str, List[Dict[str, str]]] = {}

# üî∏ Adiciona mensagem no hist√≥rico
def adicionar_ao_historico(usuario_id: int, mensagem: str, eh_usuario: bool = True):
    """
    Adiciona uma mensagem ao hist√≥rico local.

    Args:
        usuario_id (int): ID do usu√°rio.
        mensagem (str): Conte√∫do da mensagem.
        eh_usuario (bool): True se for do usu√°rio, False se for da IA.
    """
    if not usuario_id:
        return

    if usuario_id not in _historico_conversas:
        _historico_conversas[usuario_id] = []

    _historico_conversas[usuario_id].append({
        "role": "user" if eh_usuario else "assistant",
        "content": mensagem
    })

    if len(_historico_conversas[usuario_id]) > 20:
        _historico_conversas[usuario_id] = _historico_conversas[usuario_id][-20:]


# üî∏ Recupera o hist√≥rico do usu√°rio
def obter_historico_usuario(usuario_id: int, max_mensagens: int = 3) -> str:
    """
    Retorna at√© N mensagens anteriores do usu√°rio no formato texto.

    Args:
        usuario_id (int): ID do usu√°rio.
        max_mensagens (int): Quantidade m√°xima de mensagens.

    Returns:
        str: Hist√≥rico concatenado.
    """
    if not usuario_id or usuario_id not in _historico_conversas:
        logger.info(f"üìú Sem hist√≥rico para o usu√°rio {usuario_id}")
        return ""

    mensagens = _historico_conversas[usuario_id]
    perguntas = [
        m["content"] for m in reversed(mensagens)
        if m["role"] == "user"
    ][:max_mensagens]

    if not perguntas:
        logger.info(f"üìú Nenhuma pergunta relevante no hist√≥rico")
        return ""

    perguntas.reverse()
    historico = "\n".join(f"Usu√°rio: {p}" for p in perguntas)
    logger.info(f"üìú Hist√≥rico recuperado: {historico}")
    return historico


# üî∏ Salva a intera√ß√£o no Supabase
async def salvar_interacao(
    usuario_id: Optional[int],
    pergunta: str,
    resposta: str,
    categoria: str = "geral"
):
    """
    Salva pergunta, resposta e categoria no Supabase.

    Args:
        usuario_id (Optional[int]): ID do usu√°rio.
        pergunta (str): Pergunta feita.
        resposta (str): Resposta gerada.
        categoria (str): Classifica√ß√£o da pergunta.
    """
    try:
        client = get_supabase_client()

        data = {
            "usuario_id": usuario_id,
            "pergunta": pergunta,
            "resposta": resposta,
            "categoria": categoria
        }

        response = client.table("mensagens").insert(data).execute()

        if response.data:
            logger.info(f"‚úÖ Intera√ß√£o salva no Supabase para usu√°rio {usuario_id}")
        else:
            logger.warning(f"‚ö†Ô∏è Nenhum dado salvo no Supabase. Resposta: {response}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao salvar intera√ß√£o no Supabase: {e}")


# Cole esta fun√ß√£o no lugar da sua 'processar_pergunta' atual em app/services/fluxo_chat.py

# üî• Pipeline principal
async def processar_pergunta(usuario_id: Optional[int], pergunta: str) -> Dict:
    """
    Pipeline principal que processa a pergunta (Vers√£o N√≠vel 1 - Especialista Confi√°vel):
    - Classifica a inten√ß√£o.
    - Decide se precisa de RAG ou n√£o.
    - Gera resposta formatada e com fontes.
    - Salva no hist√≥rico e Supabase.
    """
    inicio = time.time()
    logger.info(f"üß† Pergunta recebida: {pergunta}")

    adicionar_ao_historico(usuario_id, pergunta, eh_usuario=True)

    # 1Ô∏è‚É£ Classifica categoria
    categoria = await classificar_pergunta(pergunta)
    logger.info(f"üìö Classifica√ß√£o da pergunta: {categoria}")

    # 2Ô∏è‚É£ Decide se precisa de RAG
    precisa_rag = await classificar_precisa_rag(pergunta)
    logger.info(f"üîç Precisa usar RAG? {precisa_rag}")

    artigos_encontrados = []
    resposta = ""

    # 3Ô∏è‚É£ Gera resposta com ou sem RAG
    if precisa_rag:
        logger.info("üöÄ Buscando artigos no Weaviate...")
        artigos_encontrados = await buscar_artigos_weaviate(pergunta, categoria=categoria)
        logger.info(f"‚úÖ {len(artigos_encontrados)} artigos encontrados.")

        # L√≥gica de RAG aprimorada
        if artigos_encontrados:
            # Monta o contexto para o LLM
            contexto = "\n\n---\n\n".join(
                [f"T√≠tulo: {a['title']}\nURL: {a['url']}\nConte√∫do: {a['content']}" for a in artigos_encontrados]
            )

            # Prepara os t√≠tulos para a cita√ß√£o de fontes
            titulos_fontes = "\n".join([f"* {a['title']}" for a in artigos_encontrados])

            # === MELHORIA N√çVEL 1: PROMPT DO SISTEMA ===
            # PROMPT FINAL E REFINADO
            system_prompt = """
            Voc√™ √© um assistente especialista no ERP Vision, amig√°vel e extremamente did√°tico. Sua miss√£o √© fornecer respostas claras e precisas baseadas exclusivamente nos artigos da base de conhecimento fornecidos.

            **Instru√ß√µes de Resposta:**
            1.  Analise a pergunta do usu√°rio e o contexto dos artigos fornecidos.
            2.  Formule uma resposta direta e √∫til. Se a pergunta for sobre "como fazer", crie um passo a passo. Se for sobre "o que √©", crie uma explica√ß√£o concisa.
            3.  Use **Markdown** para formatar a resposta (use **negrito** para destacar menus, bot√µes e conceitos importantes) para m√°xima clareza.
            4.  **IMPORTANTE: N√£o inclua os t√≠tulos dos artigos ou links no corpo da sua resposta principal.** A interface do usu√°rio cuidar√° de exibir as fontes separadamente. A sua resposta deve ser um texto limpo, coeso e aut√¥nomo.
            5.  **NUNCA** invente informa√ß√µes. Se a resposta n√£o estiver no contexto, informe que n√£o encontrou a informa√ß√£o nos artigos.
            6.  Encerre de forma amig√°vel, incentivando o usu√°rio a fazer mais perguntas caso a d√∫vida n√£o tenha sido totalmente esclarecida.
            """

            # === MELHORIA N√çVEL 1: MENSAGEM DO USU√ÅRIO ===
            # user_message ATUALIZADA (opcional, mas recomendado)
            user_message = f"""
            **Artigos da Base de Conhecimento (Contexto):**
            ---
            {contexto}
            ---

            **Pergunta do Usu√°rio:**
            "{pergunta}"
            """

            resposta = await generate_chat_completion(
                system_prompt=system_prompt,
                user_message=user_message,
                temperature=0.2,  # Baixamos a temperatura para respostas mais factuais
                max_tokens=1024   # Aumentamos um pouco para acomodar a formata√ß√£o
            )
        else:
            # Se o RAG foi acionado mas n√£o encontrou artigos, caia para a resposta padr√£o
            logger.warning("‚ö†Ô∏è RAG solicitado, mas nenhum artigo encontrado. Usando resposta padr√£o.")
            precisa_rag = False # For√ßa a entrada no bloco 'else' abaixo

    if not precisa_rag: # Ativado se o RAG n√£o for necess√°rio ou se falhou em encontrar artigos
        logger.info("üí¨ Respondendo sem RAG (conhecimento geral)...")
        system_prompt = "Voc√™ √© um assistente amig√°vel e prestativo especializado no ERP Vision. Responda √† pergunta do usu√°rio de forma clara e objetiva com base no seu conhecimento geral."
        user_message = pergunta

        resposta = await generate_chat_completion(
            system_prompt=system_prompt,
            user_message=user_message,
            temperature=0.7,
            max_tokens=800
        )

    # 4Ô∏è‚É£ Adiciona resposta ao hist√≥rico e salva intera√ß√£o
    adicionar_ao_historico(usuario_id, resposta, eh_usuario=False)
    await salvar_interacao(usuario_id, pergunta, resposta, categoria)

    tempo = round(time.time() - inicio, 2)

    # 5Ô∏è‚É£ Prepara retorno
    return {
        "resposta": resposta,
        "categoria": categoria,
        "artigos": artigos_encontrados,
        "tempo_processamento": tempo,
        "prompt_usado": "Prompt N√≠vel 1 - Especialista Confi√°vel" # Atualiza o nome do prompt
    }


async def classificar_precisa_rag(pergunta: str) -> bool:
    """
    Classifica se a pergunta precisa ou n√£o de busca RAG (Weaviate).
    Retorna True (precisa RAG) ou False (n√£o precisa).
    """

    system_prompt = "Voc√™ √© um classificador. Para cada pergunta, responda apenas com SIM ou N√ÉO, conforme a necessidade de consultar artigos no RAG (Weaviate)."

    user_prompt = f"""
    Crit√©rios:
    - Se a pergunta envolve informa√ß√µes din√¢micas, procedimentos detalhados, atualiza√ß√µes recentes ou d√∫vidas comuns de suporte ‚Üí SIM.
    - Se a pergunta √© gen√©rica, social ou um processo padr√£o bem conhecido ‚Üí N√ÉO.

    Exemplos:
    1. "Como lan√ßar uma nota fiscal?" ‚Üí N√ÉO
    2. "Como configurar o envio de XML para a contabilidade?" ‚Üí SIM
    3. "Boa tarde, tudo bem?" ‚Üí N√ÉO
    4. "Onde encontro os relat√≥rios de comiss√£o?" ‚Üí SIM
    5. "Qual o telefone do suporte?" ‚Üí N√ÉO

    Pergunta: "{pergunta}"
    Responda apenas SIM ou N√ÉO.
    """

    resposta = await generate_chat_completion(
        system_prompt=system_prompt,
        user_message=user_prompt,
        temperature=0,
        max_tokens=5
    )

    # Normaliza e interpreta
    resposta_limpa = resposta.strip().upper()

    if "SIM" in resposta_limpa:
        return True
    elif "N√ÉO" in resposta_limpa or "NAO" in resposta_limpa:
        return False
    else:
        # fallback defensivo ‚Üí se a IA n√£o responder certo, assume que n√£o precisa RAG
        return False


# üëá ESTA √â A VERS√ÉO FINAL DA FUN√á√ÉO QUE ORQUESTRA TUDO üëá
# (Substitua a vers√£o anterior que criamos)
async def buscar_artigos_weaviate(pergunta: str, categoria: str, limite: int = 3) -> list:
    """
    Orquestra a busca de artigos:
    1. Gera o embedding da pergunta.
    2. Busca os artigos no Weaviate usando o embedding e o filtro de categoria.
    """
    logger.info(f"Iniciando busca RAG para a pergunta: '{pergunta}'")
    
    # Passo 1: Gerar o embedding para a pergunta do usu√°rio
    embedding = await gerar_embedding_openai(pergunta)
    
    # Verifica se a gera√ß√£o do embedding falhou
    if embedding is None:
        logger.warning("N√£o foi poss√≠vel gerar o embedding da pergunta. Abortando busca RAG.")
        return []

    # Passo 2: Buscar artigos no Weaviate usando o embedding e a categoria
    logger.info("Buscando artigos no Weaviate com o embedding gerado...")
    artigos_encontrados = await buscar_artigos_por_embedding(
        embedding=embedding,
        categoria=categoria,
        limit=limite
    )
    
    return artigos_encontrados